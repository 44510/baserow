import json
from typing import Any, List, Dict, Tuple, TypeVar

from django.utils.encoding import force_str
from django.core.files.base import ContentFile
from django.core.exceptions import ValidationError
from django.db import transaction
from django.utils import translation
from django.conf import settings
from rest_framework import serializers

from baserow.api.exceptions import RequestBodyValidationException
from baserow.api.utils import validate_data
from baserow.contrib.database.api.rows.serializers import get_row_serializer_class
from baserow.core.utils import grouper
from baserow.core.jobs.registries import JobType
from baserow.contrib.database.table.signals import table_updated
from baserow.contrib.database.rows.handler import RowHandler

from .exceptions import FileImportMaxErrorCountExceeded
from .models import FileImportJob
from .constants import FILE_IMPORT_IN_PROGRESS, PRE_VALIDATION_IN_PROGRESS
from .serializers import (
    ReportSerializer,
)

BATCH_SIZE = 1024

RowIndex = TypeVar("RowIndex", bound=int)


def serialize_errors_recursive(error):
    if isinstance(error, dict):
        return {
            key: serialize_errors_recursive(errors) for key, errors in error.items()
        }
    elif isinstance(error, list):
        return [serialize_errors_recursive(errors) for errors in error]
    else:
        if isinstance(error, ValidationError):
            return {"error": error.message, "code": error.code}
        if isinstance(error, Exception):
            return {"error": force_str(error), "code": "unknown_error"}
        return error


def prepare_field_errors(field_errors):
    """
    Here we update the index generated by the call of the create_rows method because
    that method received only valid rows so the index might be incorrect.
    """

    if not field_errors:
        return None

    return {
        field: serialize_errors_recursive(errs) for field, errs in field_errors.items()
    }


class RowErrorReport:
    def __init__(
        self,
        rows: List[Dict[str, Any]],
        error_limit: int = settings.BASEROW_MAX_FILE_IMPORT_ERROR_COUNT,
    ):
        """
        The RowErrorReport is a helper to track rows errors and generate a report at
        the end.

        :param rows: the rows list.
        :param error_limit: if the error limit is exceeded, an exception is raised.
        """

        self._indexed_rows = {
            index: {"row": row, "error": None} for index, row in enumerate(rows)
        }
        self._rows = rows
        self.error_count = 0
        self.error_limit = error_limit

    def add_error(self, row_index: RowIndex, error: Dict[str, Any]):
        """
        Adds an error to the report if the error is truthy.

        :raise FileImportMaxErrorCountExceeded: if the maximum error limit is exceeded.
        """

        if not error:
            return

        self.error_count += 1
        if self.error_count > self.error_limit:
            raise FileImportMaxErrorCountExceeded(self.to_dict())

        self._indexed_rows[row_index]["error"] = error

    def get_valid_rows_and_mapping(
        self,
    ) -> Tuple[List[Dict[str, Any]], Dict[RowIndex, RowIndex]]:
        """
        Returns rows without error and the corresponding mapping for
        new original -> original index
        """

        valid_rows = []
        mapping = {}
        for index, row in enumerate(self._rows):
            if not self._indexed_rows[index]["error"]:
                mapping[len(valid_rows)] = index
                valid_rows.append(row)
        return valid_rows, mapping

    def to_dict(self) -> Dict[RowIndex, Dict[str, Any]]:
        """
        Generates the report as a dict.
        """

        report = {}
        for index, _ in enumerate(self._rows):
            if self._indexed_rows[index]["error"]:
                report[index] = self._indexed_rows[index]["error"]
        return report


class FileImportJobType(JobType):
    type = "file_import"
    model_class = FileImportJob
    max_count = 1
    request_serializer_field_names = []
    request_serializer_field_overrides = {}

    job_exceptions_map = {
        FileImportMaxErrorCountExceeded: f"This file import has raised too many "
        "errors.",
    }

    serializer_field_names = ["table_id", "report"]

    serializer_field_overrides = {
        "table_id": serializers.IntegerField(
            required=True,
            help_text="Table id where data will be imported.",
        ),
        "report": ReportSerializer(help_text="Import error report."),
    }

    def prepare_values(self, values, user):
        """
        Filter data from the values dict. Data are going to be added later as a file.
        See `.after_job_creation()`.
        """

        filtered_dict = dict(**values)
        filtered_dict.pop("data")
        return filtered_dict

    def after_job_creation(self, job, values):
        """
        Save the data file for the newly created job.
        """

        data_file = ContentFile(json.dumps(values["data"]))
        job.data_file.save(None, data_file)

    def before_delete(self, job):
        """
        Try to delete the data file of a job before deleting the job.
        """

        try:
            job.data_file.delete()
        except ValueError:
            # File doesn't exist, that's ok
            pass

    def on_error(self, job, error):
        if isinstance(error, FileImportMaxErrorCountExceeded):
            job.report = {"failing_rows": error.report}
            job.save(update_fields=("report",))

    def run(self, job, progress):
        """
        Fills the provided table with the normalized data that needs to be created upon
        creation of the table.
        """

        data = []

        group = job.table.database.group
        group.has_user(job.user, raise_error=True)

        with job.data_file.open("r") as fin:
            data = json.load(fin)

        fields = list(job.table.field_set.all())

        # Here we assume the data has already been checked for shape and row length by
        # the job initiator.
        rows = [
            {f"field_{fields[index].id}": value for index, value in enumerate(row)}
            for row in data
        ]

        error_report = RowErrorReport(rows)
        # We want error messages to be translated with the user locale if possible
        with translation.override(job.user.profile.language):

            validation_sub_progress = progress.create_child(50, len(rows))
            validation_sub_progress.increment(state=PRE_VALIDATION_IN_PROGRESS)

            # STEP 1: pre-validate data with serializer
            model = job.table.get_model()
            validation_serializer = get_row_serializer_class(model)
            for count, chunk in enumerate(grouper(BATCH_SIZE, rows)):
                row_start_index = count * BATCH_SIZE
                try:
                    validate_data(validation_serializer, list(chunk), many=True)
                except RequestBodyValidationException as e:
                    # Add errors to global report
                    for index, err in enumerate(e.detail["detail"]):
                        error_report.add_error(row_start_index + index, err)

                validation_sub_progress.increment(len(chunk))

            creation_sub_progress = progress.create_child(50, len(data))
            creation_sub_progress.increment(state=FILE_IMPORT_IN_PROGRESS)

            (
                valid_rows,
                original_row_index_mapping,
            ) = error_report.get_valid_rows_and_mapping()

            # STEP 2: create rows in DB
            row_handler = RowHandler()
            for count, chunk in enumerate(grouper(BATCH_SIZE, valid_rows)):
                row_start_index = count * BATCH_SIZE
                _, creation_report = row_handler.create_rows(
                    user=job.user,
                    table=job.table,
                    model=model,
                    rows_values=chunk,
                    generate_error_report=True,
                    send_signal=False,
                )
                creation_sub_progress.increment(len(chunk))

                # Add errors to global report
                for valid_index, field_errors in creation_report.items():
                    error_report.add_error(
                        original_row_index_mapping[int(valid_index) + row_start_index],
                        prepare_field_errors(field_errors),
                    )

        def after_commit():
            """
            Removes the data file to save space and save the error report.
            """

            job.refresh_from_db()
            job.data_file.delete()
            job.report = {"failing_rows": error_report.to_dict()}
            job.save()

        transaction.on_commit(after_commit)

        table_updated.send(self, table=job.table, user=None, force_table_refresh=True)
